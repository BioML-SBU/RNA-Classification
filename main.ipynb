{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pandas numpy torch matplotlib tqdm pytorch-lightning torchmetrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor , ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"data\\ECCB2017\\dataset_Rfam_6320_13classes.fasta\"\n",
    "VAL_DATA_PATH = \"data\\ECCB2017\\dataset_Rfam_validated_2600_13classes.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(filepath):\n",
    "    try:\n",
    "        sequences = {}\n",
    "        with open(filepath, 'r') as f:\n",
    "            name = None\n",
    "            seq = ''\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('>'):\n",
    "                    if name:\n",
    "                        sequences[name] = seq\n",
    "                    name = line[1:]\n",
    "                    seq = ''\n",
    "                else:\n",
    "                    seq += line\n",
    "            if name:\n",
    "                sequences[name] = seq\n",
    "        return sequences\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_sequence(sequence):\n",
    "    return ''.join(c.upper() for c in sequence if c in 'AUCG')\n",
    "\n",
    "def extract_rna_type(name):\n",
    "    parts = name.split()\n",
    "    return parts[0] , parts[1]\n",
    "\n",
    "def process_fasta_data(fasta_data):\n",
    "    processed_data = []\n",
    "    for name, seq in fasta_data.items():\n",
    "        cleaned_seq = clean_sequence(seq)\n",
    "        name , rna_type = extract_rna_type(name)\n",
    "        processed_data.append((cleaned_seq, name,rna_type))\n",
    "    return processed_data\n",
    "\n",
    "def load_and_process_fasta(filepath):\n",
    "    raw_data = read_fasta(filepath)\n",
    "    if raw_data is None:\n",
    "        return None\n",
    "    processed_data = process_fasta_data(raw_data)\n",
    "    return pd.DataFrame(processed_data, columns=['Sequence','Name','RNA_Type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6320, 2600)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = load_and_process_fasta(TRAIN_DATA_PATH)\n",
    "test_df = load_and_process_fasta(VAL_DATA_PATH)\n",
    "len(set(train_df['Sequence'].tolist())) , len(set(test_df['Sequence'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNADataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.label_to_index = {label: idx for idx, label in enumerate(dataframe['RNA_Type'].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.dataframe.iloc[idx]['Sequence']\n",
    "        label = self.dataframe.iloc[idx]['RNA_Type']\n",
    "        label_encoded = self.label_to_index[label]\n",
    "        return sequence, label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'A': 1,\n",
    "    'U': 2,\n",
    "    'C': 3,\n",
    "    'G': 4,\n",
    "    '<UNK>':-1,\n",
    "    '<PAD>': 0\n",
    "}\n",
    "\n",
    "def encode_sequence(sequence):\n",
    "    return [vocab.get(char, vocab['<UNK>']) for char in sequence]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    encoded = [encode_sequence(seq) for seq in sequences]\n",
    "    \n",
    "    max_len = max(len(seq) for seq in encoded)\n",
    "    padded = [\n",
    "        seq + [vocab['<PAD>']] * (max_len - len(seq)) for seq in encoded\n",
    "    ]\n",
    "    \n",
    "    sequences_tensor = torch.tensor(padded, dtype=torch.long)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return sequences_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RNADataset(train_df)\n",
    "trainloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "test_dataset = RNADataset(train_df)\n",
    "testloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleBiGRU(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, output_dim: int, vocab_size: int, embedding_dim: int):\n",
    "        super(SimpleBiGRU, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab['<PAD>'])\n",
    "        self.gru = nn.GRU(input_size=embedding_dim,\n",
    "                          hidden_size=hidden_dim,\n",
    "                          batch_first=True,\n",
    "                          bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Multiply by 2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_length, embedding_dim)\n",
    "        gru_out, _ = self.gru(embedded)  # Shape: (batch_size, seq_length, hidden_dim * 2)\n",
    "        pooled = F.adaptive_avg_pool1d(gru_out.permute(0, 2, 1), 1).squeeze(2)\n",
    "        output = self.fc(pooled)  # Shape: (batch_size, output_dim)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAClassifier(pl.LightningModule):\n",
    "    def __init__(self, hidden_dim, output_dim, vocab_size, embedding_dim, learning_rate):\n",
    "        super().__init__()\n",
    "        self.model = SimpleBiGRU(hidden_dim, output_dim, vocab_size, embedding_dim)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task='multiclass',num_classes=13)\n",
    "        self.val_acc = torchmetrics.Accuracy(task='multiclass',num_classes=13)\n",
    "        self.val_f1 = torchmetrics.F1Score(task='multiclass',num_classes=13)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        sequences, labels = batch\n",
    "        outputs = self(sequences)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        self.train_acc(outputs, labels.int())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        sequences, labels = batch\n",
    "        outputs = self(sequences)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        self.val_acc(outputs, labels.int())\n",
    "        self.val_f1(outputs, labels.int())\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, fused=True)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "hidden_dim = 128\n",
    "output_dim = 13\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "learning_rate = 5e-4\n",
    "\n",
    "model = RNAClassifier(hidden_dim, output_dim, vocab_size, embedding_dim, learning_rate)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',       # Metric to monitor\n",
    "    save_top_k=1,             # Save only the best model\n",
    "    mode='min',               # Mode 'min' because lower validation loss is better\n",
    "    filename='best-checkpoint', # Filename for the checkpoint\n",
    "    verbose=False              # Enable logging for this callback\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',       # Metric to monitor\n",
    "    patience=30,              # Number of epochs with no improvement after which training will be stopped\n",
    "    mode='min',               # Mode 'min' because lower validation loss is better\n",
    "    verbose=False              # Enable logging for this callback\n",
    ")\n",
    "\n",
    "lr_scheduler_callback = LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    # strategy='ddp_notebook',  # Use 'ddp_notebook' for Jupyter environments\n",
    "    callbacks=[lr_scheduler_callback,checkpoint_callback, early_stopping_callback]\n",
    ")\n",
    "\n",
    "trainer.fit(model, trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
